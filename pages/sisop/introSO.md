---
layout: cover
level: 2
title: Cosa succede quando un programma viene eseguito?

---

# üßë‚Äçüéì

Studente

Quindi cosa succede quando un programma viene eseguito?

---
layout: cover
level: 3
title: Welcome

---

# üßë‚Äçüè´

Professore

Beh, un programma in esecuzione fa una cosa molto semplice: esegue istruzioni.

Molti milioni (e oggi, anche miliardi) di volte ogni secondo, il **processore**

1. preleva un'istruzione dalla memoria,
2. la decodifica (cio√®, capisce quale istruzione √®)
3. la esegue (cio√®, fa la cosa che dovrebbe fare, come sommare due numeri, accedere alla memoria, controllare una condizione, saltare a una funzione e cos√¨ via).
4. Dopo aver terminato questa istruzione, il processore passa all'istruzione successiva, e cos√¨ via, e cos√¨ via, fino a quando il programma non si completa.

---
layout: cover
level: 3
title: Welcome
---

# üßë‚Äçüéì

Studente

Quindi, abbiamo appena descritto le basi del modello di calcolo di [Von Neumann.](http://fabrizio.phpnet.us/ilcomputer/?page=1)

Sembra semplice, giusto?

---

## Microprocessori CPU  

<div class="scrollable">

MIPS (Microprocessor without Interlocked Pipeline Stages) e x86 sono due diverse architetture di set di istruzioni (Instruction Set Architecture, ISA) utilizzate nei processori. Ecco una panoramica delle principali differenze tra MIPS e x86:

### Architettura e Design

**MIPS:**
- **RISC (Reduced Instruction Set Computer)**: MIPS √® un esempio classico di architettura RISC, che utilizza un set di istruzioni ridotto e semplice.
- **Filosofia del Design**: Mira a migliorare le prestazioni riducendo la complessit√† delle istruzioni, con istruzioni uniformi e operazioni di memoria semplici.
- **Pipelines**: √à progettato per avere un'implementazione pipeline efficiente, con meno dipendenze tra istruzioni.
- **Registri**: Tipicamente ha un set di registri ampio (es., 32 registri generali), il che riduce la necessit√† di operazioni di memoria.
- **Uniformit√† delle Istruzioni**: Le istruzioni MIPS sono tutte della stessa lunghezza (tipicamente 32 bit), il che semplifica la decodifica.

**x86:**
- **CISC (Complex Instruction Set Computer)**: x86 √® un esempio di architettura CISC, che utilizza un set di istruzioni pi√π ampio e complesso.
- **Filosofia del Design**: Offre un'ampia gamma di istruzioni, incluse molte operazioni complesse che possono eseguire pi√π azioni in una singola istruzione.
- **Pipelines**: La complessit√† delle istruzioni rende la pipeline pi√π complicata, con meccan7ismi avanzati per gestire le dipendenze.
- **Registri**: Ha un numero minore di registri generali (originariamente 8, estesi a 16 nei moderni processori a 64 bit), il che pu√≤ portare a un maggior uso di operazioni di memoria.
- **Variabilit√† delle Istruzioni**: Le istruzioni x86 possono avere lunghezze variabili (da 1 a 15 byte), rendendo la decodifica pi√π complessa.

### Applicazioni e Utilizzo

**MIPS:**
- **Settori di Utilizzo**: Comunemente usato in sistemi embedded, router, dispositivi di rete, console di gioco e dispositivi elettronici.
- **Semplicit√† e Efficienza**: L'architettura semplice e le istruzioni uniformi rendono MIPS adatto a progetti che richiedono efficienza energetica e velocit√† di esecuzione.

**x86:**
- **Settori di Utilizzo**: Dominante nei computer desktop, laptop, server, workstation, e applicazioni di calcolo generico.
- **Compatibilit√† e Potenza**: La compatibilit√† con un vasto ecosistema di software e hardware lo rende molto versatile e potente per applicazioni complesse e di alto livello.

### Evoluzione e Compatibilit√†

**MIPS:**
- **Evoluzione**: √à rimasto relativamente stabile nel tempo, mantenendo la filosofia RISC di base.
- **Compatibilit√†**: Le versioni successive mantengono la compatibilit√† con le versioni precedenti, ma meno focalizzate sull'espansione verso caratteristiche complesse.

**x86:**
- **Evoluzione**: Ha subito numerose evoluzioni, aggiungendo estensioni come MMX, SSE, AVX e altre per migliorare le prestazioni grafiche e multimediali.
- **Compatibilit√†**: Mantiene una rigorosa compatibilit√† retroattiva, permettendo l'esecuzione di software legacy su hardware moderno.

### Prestazioni e Efficienza

**MIPS:**
- **Efficienza**: Progettato per efficienza e velocit√†, con un focus su pipeline e prestazioni per ciclo di clock.
- **Consumo Energetico**: Tendenzialmente pi√π efficiente dal punto di vista energetico, il che lo rende ideale per dispositivi portatili e embedded.

**x86:**
- **Prestazioni**: Progettato per prestazioni elevate e capacit√† di eseguire compiti complessi, spesso a scapito dell'efficienza energetica.
- **Consumo Energetico**: I processori x86 possono essere meno efficienti dal punto di vista energetico rispetto ai RISC, specialmente in ambienti dove l'energia √® un fattore critico.

### Sintesi delle Differenze

- **Tipo di Architettura**: MIPS √® RISC, x86 √® CISC.
- **Set di Istruzioni**: MIPS ha un set ridotto e uniforme, x86 ha un set ampio e complesso con lunghezze variabili.
- **Uso dei Registri**: MIPS ha pi√π registri generali, x86 ne ha meno ma ha introdotto estensioni per compensare.
- **Applicazioni Tipiche**: MIPS √® usato in sistemi embedded e dispositivi specifici, x86 √® dominante nei computer generici.
- **Efficienza Energetica**: MIPS √® generalmente pi√π efficiente dal punto di vista energetico, x86 √® pi√π potente ma meno efficiente.

In conclusione, la scelta tra MIPS e x86 dipende dalle specifiche esigenze del progetto, bilanciando tra prestazioni, efficienza energetica, compatibilit√† e complessit√† delle istruzioni.
</div>


<style>
.scrollable {
  max-height: 70vh; /* Altezza massima dell'area scrollabile */
  overflow-y: auto;  /* Abilita lo scroll verticale */
  padding: 1em;      /* Aggiungi un po' di padding */
  border: 1px solid #ccc; /* Aggiungi un bordo per evidenziare l'area scrollabile */
}
</style>

Sembra semplice, giusto?


---
layout: cover
level: 3
title: Welcome

---

# üßë‚Äçüè´

Professore

Ma in questa classe, impareremo che mentre un programma √® in esecuzione, molte altre cose selvagge stanno accadendo con l'obiettivo principale di rendere il sistema facile da usare.

Von Neumann √® stato uno dei primi pionieri dei sistemi informatici.

Ha anche svolto un lavoro pionieristico sulla teoria dei giochi e sulle bombe atomiche, e ha giocato nella NBA per sei anni. 

Ok, una di queste cose non √® vera.

Il codice fornito √® un esempio di come creare e gestire thread in un programma C utilizzando la libreria POSIX Threads, comunemente nota come Pthreads. 

Questo frammento di codice dimostra la creazione di un numero definito di thread, ognuno dei quali eseguir√† una funzione specifica.

---
layout: cover
level: 3
title: Welcome

---

# üßë‚Äçüéì

Studente

Certo, i processori moderni fanno molte cose bizzarre e spaventose sotto il cofano per far funzionare i programmi pi√π velocemente, ad esempio, eseguendo pi√π istruzioni contemporaneamente e persino emettendole e completandole fuori ordine! 

Ma questo non √® il nostro problema qui; siamo solo interessati al semplice modello che la maggior parte dei programmi assume: che le istruzioni apparentemente si eseguono una alla volta, in modo ordinato e sequenziale.

---
layout: cover
level: 3
title: Welcome

---

# üßë‚Äçüè´

Professore

Esiste un insieme di software, in realt√†, che √® responsabile di rendere facile l'esecuzione dei programmi (consentendoti persino di eseguirne apparentemente molti contemporaneamente), consentendo ai programmi di condividere la memoria, consentendo ai programmi di interagire con i dispositivi e altre cose divertenti del genere.

Questo insieme di software √® chiamato **sistema operativo (OS)**, in quanto √® responsabile di garantire che il sistema funzioni correttamente ed efficientemente in modo facile da usare.

---
layout: cover
level: 3
title: Welcome

---

# üßë‚Äçüè´

Professore

Il modo principale in cui l'OS lo fa √® attraverso una tecnica generale che chiamiamo **virtualizzazione**. 

Cio√®, l'OS prende una risorsa fisica (come il processore, o la memoria, o un disco) e la trasforma in una forma virtuale pi√π generale, potente e facile da usare. 

Pertanto, a volte ci riferiamo al sistema operativo come a una macchina virtuale. Naturalmente, per consentire agli utenti di dire all'OS cosa fare e quindi utilizzare le funzionalit√† della macchina virtuale (come l'esecuzione di un programma, o l'allocazione della memoria, o l'accesso a un file), l'OS fornisce anche alcune interfacce (API) che puoi chiamare.

---
layout: cover
level: 3
title: Welcome

---

# üßë‚Äçüè´

Professore

Un tipico OS, infatti, esporta alcune centinaia di chiamate di sistema che sono disponibili per le applicazioni.

Poich√© l'OS fornisce queste chiamate per eseguire programmi, accedere alla memoria e ai dispositivi e altre azioni correlate, a volte diciamo anche che l'OS fornisce una libreria standard alle applicazioni.

---
layout: cover
level: 3
title: Welcome

---

# üßë‚Äçüè´

Professore

Infine, poich√© la virtualizzazione consente a molti programmi di essere eseguiti (condividendo quindi la CPU), e a molti programmi di accedere contemporaneamente alle proprie istruzioni e dati (condividendo quindi la memoria), e a molti programmi di accedere ai dispositivi (condividendo quindi dischi e cos√¨ via), l'OS √® a volte noto come gestore di risorse. 

Ogni CPU, memoria e disco √® una risorsa del sistema; √® quindi compito del sistema operativo gestire tali risorse, farlo in modo efficiente  equo o con molti altri possibili obiettivi in mente.

---
layout: cover
level: 3
title: Welcome

---

# üßë‚Äçüè´

Professore

Un altro nome iniziale per il sistema operativo era il supervisore o anche il programma di controllo principale. Apparentemente, quest'ultimo suonava un po' troppo zelante (vedi il film Tron per i dettagli) e quindi, fortunatamente, "sistema operativo" √® diventato popolare invece.

<div class="scrollable">
## Tron

Il film "Tron" del 1982, diretto da Steven Lisberger, racconta la storia di Kevin Flynn, un giovane e geniale programmatore.
Flynn si ritrova intrappolato in un mondo virtuale dove i programmi sono personaggi viventi e deve lottare contro un'intelligenza artificiale malvagia chiamata "Master Control Program".
Con l'aiuto di Tron, un programma di sicurezza, Flynn cerca di sconfiggere il MCP e tornare nel mondo reale. Il film mescola elementi di fantascienza e azione, offrendo uno sguardo affascinante sulle possibilit√† e i pericoli della tecnologia.

Nel film Tron, il concetto di "master control program" √® associato a un'intelligenza artificiale malvagia e autoritaria che controlla il mondo virtuale all'interno del film.

Quindi, quando si fa riferimento al termine "master control program" come nome per il sistema operativo, potrebbe essere considerato eccessivamente zelante o autoritario, proprio a causa di questa associazione con il personaggio negativo del film Tron.

</div>


<style>
.scrollable {
  max-height: 50vh; /* Altezza massima dell'area scrollabile */
  overflow-y: auto;  /* Abilita lo scroll verticale */
  padding: 1em;      /* Aggiungi un po' di padding */
  border: 1px solid #ccc; /* Aggiungi un bordo per evidenziare l'area scrollabile */
}
</style>


---
layout: center
level: 3
title: Welcome

---

## IL NOCCIOLO DEL PROBLEMA: COME VIRTUALIZZARE LE RISORSE

Una domanda centrale a cui risponderemo √® piuttosto semplice:
**Come il sistema operativo virtualizza le risorse?**
Questo √® il nocciolo del nostro problema.
Perch√© il sistema operativo fa questo non √® la domanda principale, poich√© la risposta dovrebbe essere ovvia: rende il sistema pi√π facile da usare.

---
layout:
level: 3
title: Welcome

--- 

## Quindi, ci concentriamo sul come:

- Quali meccanismi e politiche sono implementati dal sistema operativo per raggiungere la virtualizzazione?
- Come fa il sistema operativo a farlo in modo efficiente?
- Quale hardware di supporto √® necessario?**

---
layout:
level: 3
title: Welcome

---

Per capire un po' meglio il ruolo dell'OS, diamo un'occhiata ad alcuni esempi.

1. [helloCpu.c](helloCpu.c)
Il nostro primo programma. Non fa molto. Infatti, tutto ci√≤ che fa √® chiamare Spin(), una funzione che controlla ripetutamente l'ora e ritorna una volta che √® passato un secondo. Poi, stampa la stringa che l'utente ha passato sulla linea di comando, e ripete all'infinito.

---
layout:
level: 3
title: Welcome

---

Supponiamo di salvare questo file come helloCpu.c e decidiamo di compilarlo ed eseguirlo su un sistema con un singolo processore (o CPU come a volte lo chiameremo). Ecco cosa vedremo:

```
prompt> gcc -o cpu helloCpu.c -Wall
prompt> ./cpu A
A
A
A
A
ÀÜC
prompt>
```

---
layout:
level: 3
title: Welcome

---

## Non √® una run molto interessante 

- il sistema inizia ad eseguire il programma, che controlla ripetutamente l'ora fino a che non √® passato un secondo. 
- Una volta passato un secondo, il codice stampa la stringa di input passata dall'utente (in questo esempio, la lettera "A"), e continua.
  
Nota che il programma continuer√† a funzionare all'infinito; premendo "Control-c" (che nei sistemi basati su UNIX terminer√† il programma in esecuzione in primo piano) possiamo interrompere il programma.

---
layout:
level: 3
title: Welcome

---
## Ora, facciamo la stessa cosa, ma questa volta eseguiamo molte istanze diverse dello stesso programma.


```
prompt> ./cpu A & ./cpu B & ./cpu C & ./cpu D
A
B
C
D
A
B
.
.
```

Nota come abbiamo eseguito quattro processi contemporaneamente, utilizzando il simbolo &. Facendo cos√¨, si esegue un lavoro in background nella shell zsh, il che significa che l'utente √® in grado di emettere immediatamente il comando successivo, che in questo caso √® un altro programma da eseguire. Se stai usando una shell diversa (ad esempio, tcsh), funziona in modo leggermente diverso; leggi la documentazione online per i dettagli.

---
layout:
level: 3
title: Welcome

---

Bene, ora le cose diventano un po' pi√π interessanti. Anche se abbiamo solo un processore, in qualche modo tutti e quattro questi programmi sembrano essere eseguiti contemporaneamente! Come avviene questa magia?

---
layout:
level: 3
title: Welcome

---

Si scopre che il sistema operativo, con un po' di aiuto dall'hardware, √® responsabile di questa illusione, ovvero l'illusione che il sistema abbia un numero molto grande di CPU virtuali. Trasformare una singola CPU (o un piccolo insieme di esse) in un numero apparentemente infinito di CPU e quindi permettere a molti programmi di sembrare essere eseguiti contemporaneamente √® ci√≤ che chiamiamo **virtualizzazione della CPU**.

---
layout:
level: 3
title: Welcome

---

Naturalmente, per eseguire programmi, fermarli e altrimenti dire al sistema operativo quali programmi eseguire, devono esserci delle interfacce (API) che si possono usare per comunicare i propri desideri al sistema operativo.

Parleremo di queste API; infatti, sono il modo principale in cui la maggior parte deglla prima partei utenti interagisce con i sistemi operativi.

---
layout:
level: 3
title: Welcome

---

Potresti anche notare che la possibilit√† di eseguire pi√π programmi contemporaneamente solleva tutta una serie di nuove domande. 

Ad esempio, se due programmi vogliono essere eseguiti in un determinato momento, quale deve essere eseguito? 

Questa domanda trova risposta in una politica del sistema operativo; le politiche sono utilizzate in molti diversi punti all'interno di un sistema operativo per rispondere a questo tipo di domande, e quindi le studieremo mentre apprendiamo i meccanismi di base che i sistemi operativi implementano (come la capacit√† di eseguire pi√π programmi contemporaneamente).

Da qui il ruolo del sistema operativo come gestore delle risorse.

---
layout:
level: 3
title: La memoria virtuale

---

## La memoria virtuale

Ora consideriamo la memoria.

Il modello di memoria fisica presentato dalle macchine moderne √® molto semplice.

La memoria √® solo una matrice di byte; per leggere la memoria, bisogna specificare un indirizzo per poter accedere ai dati memorizzati; per scrivere (o aggiornare) la memoria, bisogna anche specificare i dati da scrivere all'indirizzo dato.

---
layout:
level: 3
title: accesso alla memoria 

---

La memoria viene acceduta continuamente quando un programma √® in esecuzione. Un programma mantiene tutte le sue strutture di dati in memoria e vi accede tramite varie istruzioni, come load e store o altre istruzioni esplicite che accedono alla memoria per svolgere il loro lavoro. 

Non dimenticare che ogni istruzione del programma √® anch'essa in memoria; quindi la memoria viene acceduta a ogni recupero di istruzione.

---
layout:
level: 3
title: accesso alla memoria 

---

Diamo un'occhiata a un programma che alloca della memoria chiamando malloc().

L'output di questo programma pu√≤ essere trovato qui.
Il programma fa un paio di cose. Prima, alloca della memoria (riga a1). Poi, stampa l'indirizzo della memoria (a2) e successivamente mette il numero zero nel primo slot della memoria appena allocata (a3). Infine, entra in un ciclo, ritardando per un secondo e incrementando il valore memorizzato all'indirizzo contenuto in p. Con ogni istruzione di stampa, stampa anche quello che viene chiamato identificatore di processo (PID) del programma in esecuzione. 

Questo PID √® unico per ogni processo in esecuzione.

---
layout:
level: 3
title: accesso alla memoria 

---

Anche in questo caso, il primo risultato non √® molto interessante. La memoria appena allocata si trova all'indirizzo 0x200000. Mentre il programma viene eseguito, aggiorna lentamente il valore e stampa il risultato.

---
layout:
level: 3
title: accesso alla memoria 

---

Ora, eseguiamo nuovamente pi√π istanze di questo stesso programma per vedere cosa succede (Figura 2.4). Vediamo dall'esempio che ogni programma in esecuzione ha allocato memoria allo stesso indirizzo (0x200000), eppure ognuno sembra aggiornare il valore a 0x200000 in modo indipendente! √à come se ogni programma in esecuzione avesse la propria memoria privata, invece di condividere la stessa memoria fisica con altri programmi in esecuzione.

---
layout:
level: 3
title: accesso alla memoria 

---

In effetti, √® esattamente ci√≤ che sta accadendo qui poich√© il sistema operativo sta virtualizzando la memoria. Ogni processo accede al proprio spazio di indirizzi virtuale privato (a volte chiamato semplicemente spazio di indirizzi), che il sistema operativo in qualche modo mappa sulla memoria fisica della macchina. Un riferimento di memoria all'interno di un programma in esecuzione non influenza lo spazio di indirizzi di altri processi (o del sistema operativo stesso); per quanto riguarda il programma in esecuzione, esso ha la memoria fisica tutta per s√©. La realt√†, tuttavia, √® che la memoria fisica √® una risorsa condivisa, gestita dal sistema operativo. Esattamente come tutto questo viene realizzato √® anche il tema della prima parte di questo libro, sull'argomento della virtualizzazione.

---
layout:
level: 3
title: La persistenza

---

## La persistenza

Nella memoria di sistema, i dati possono essere facilmente persi, poich√© dispositivi come la DRAM memorizzano i valori in modo volatile; quando manca l'alimentazione o il sistema si blocca, qualsiasi dato in memoria viene perso. 

Pertanto, abbiamo bisogno di hardware e software in grado di memorizzare i dati in modo persistente; tale memorizzazione √® quindi fondamentale per qualsiasi sistema poich√© agli utenti interessa molto dei loro dati.


---
layout:
level: 3
title: La persistenza

---

L'hardware si presenta sotto forma di un qualche tipo di dispositivo di input/output o I/O; nei sistemi moderni, il disco a stato solido (SSD)  √® un repository comune per le informazioni a lungo termine.


---
layout:
level: 3
title: La persistenza

---

Il software nel sistema operativo che di solito gestisce il disco √® chiamato file system; esso √® quindi responsabile della memorizzazione di qualsiasi file creato dall'utente in modo affidabile ed efficiente sui dischi del sistema.


---
layout:
level: 3
title: La persistenza

---

A differenza delle astrazioni fornite dal sistema operativo per la CPU e la memoria, il sistema operativo non crea un disco virtuale privato per ogni applicazione. 

Piuttosto, si presume che spesso gli utenti vorranno condividere le informazioni che si trovano nei file.


---
layout:
level: 3
title: La persistenza

---

Ad esempio, quando si scrive un programma in C, si potrebbe prima usare un editor (ad esempio, Emacs) per creare e modificare il file C (emacs -nw main.c). 

Una volta terminato, si potrebbe usare il compilatore per trasformare il codice sorgente in un eseguibile (ad esempio, gcc -o main main.c). Quando si √® finito, si potrebbe eseguire il nuovo eseguibile (ad esempio, ./main). Cos√¨, √® possibile vedere come i file siano condivisi tra diversi processi. Prima, Emacs crea un file che serve come input per il compilatore; il compilatore utilizza quel file di input per creare un nuovo file eseguibile (in molti passaggi ‚Äî segui un corso di compilatori per i dettagli); infine, il nuovo eseguibile viene eseguito. E cos√¨ nasce un nuovo programma!


---
layout:
level: 3
title: La persistenza

---

Per capire meglio, diamo un'occhiata a un po' di codice. La figura 2.6 presenta il codice per creare un file (/tmp/file) che contiene la stringa "hello world". Per realizzare questo compito, il programma effettua tre chiamate nel sistema operativo. La prima, una chiamata a open(), apre il file e lo crea; la seconda, write(), scrive alcuni dati nel file; la terza, close(), chiude semplicemente il file indicando che il programma non scriver√† pi√π dati su di esso. Queste chiamate di sistema sono indirizzate alla parte del sistema operativo chiamata file system, che gestisce le richieste e restituisce un qualche tipo di codice di errore all'utente.


---
layout:
level: 3
title: La persistenza

---

Potresti chiederti cosa fa il sistema operativo per effettivamente scrivere sul disco. Te lo mostreremmo, ma dovresti promettere di chiudere gli occhi prima; √® davvero sgradevole.

Il file system deve fare un bel po' di lavoro: prima determinare dove su disco risiederanno i nuovi dati e poi tenerne traccia in varie strutture che il file system mantiene. Fare ci√≤ richiede l'emissione di richieste I/O al dispositivo di archiviazione sottostante, per leggere strutture esistenti o aggiornarle (scriverle). 

---
layout:
level: 3
title: La persistenza

---

Come chiunque abbia scritto un driver di dispositivo sa, far fare qualcosa a un dispositivo per tuo conto √® un processo intricato e dettagliato. 

Richiede una profonda conoscenza dell'interfaccia del dispositivo a basso livello e delle sue esatte semantiche.

Fortunatamente, il sistema operativo fornisce un modo standard e semplice per accedere ai dispositivi attraverso le sue chiamate di sistema. Pertanto, il sistema operativo √® talvolta visto come una libreria standard.

---
layout:
level: 3
title: La persistenza

---

Ovviamente, ci sono molti pi√π dettagli su come si accede ai dispositivi e su come i file system gestiscono i dati in modo persistente sopra detti dispositivi. Per motivi di prestazioni, la maggior parte dei file system ritarda tali scritture per un po', sperando di raggrupparle in gruppi pi√π grandi. Per gestire i problemi di crash del sistema durante le scritture, la maggior parte dei file system incorpora qualche tipo di protocollo di scrittura complesso, come il journaling o il copy-on-write, ordinando attentamente le scritture sul disco per garantire che, se si verifica un guasto durante la sequenza di scrittura, il sistema possa riprendersi a uno stato ragionevole successivamente. Per rendere efficienti diverse operazioni comuni, i file system impiegano molte strutture di dati e metodi di accesso diversi, dalle semplici liste ai complessi b-tree. Se tutto ci√≤ non ha ancora senso, bene! Parleremo di tutto questo in modo molto pi√π approfondito nella terza parte della persistenza, dove discuteremo dei dispositivi e dell'I/O in generale, e poi di dischi, RAID e file system in grande dettaglio.

Il file system √® la parte del sistema operativo responsabile della gestione dei dati persistenti. Quali tecniche sono necessarie per farlo correttamente?

Quali meccanismi e politiche sono necessari per farlo con alte prestazioni?

Come viene ottenuta l'affidabilit√†, di fronte a guasti hardware e software?

### Tecniche necessarie per la corretta gestione dei dati persistenti:
1. **Journaling**: Consiste nel mantenere un registro delle operazioni da eseguire, permettendo di ripristinare il sistema in uno stato consistente dopo un crash.
2. **Copy-on-Write (COW)**: Tecnica in cui i dati vengono copiati in una nuova posizione prima di essere modificati, garantendo che i dati originali rimangano intatti fino a quando la scrittura non √® completa.
3. **Checkpointing**: Salvataggio periodico dello stato del file system per facilitare il recupero in caso di guasto.

### Meccanismi e politiche per alte prestazioni

1. **Caching**: Memorizzazione temporanea dei dati frequentemente accessi per ridurre i tempi di accesso.
2. **Batching di Scritture**: Accumulare le scritture per ridurre il numero di operazioni I/O.
3. **Data Striping**: Distribuzione dei dati su pi√π dischi per migliorare le prestazioni di lettura/scrittura.
4. **Accesso Diretto alla Memoria (DMA)**: Utilizzo di DMA per trasferire dati direttamente tra memoria e disco senza coinvolgere la CPU, migliorando cos√¨ l'efficienza.

### Affidabilit√† di fronte a guasti hardware e software

1. **RAID (Redundant Array of Independent Disks)**: Utilizzo di diverse configurazioni RAID per ridondanza e tolleranza ai guasti.
2. **Tecniche di Redundancy**: Implementazione di tecniche come mirroring (duplicazione dei dati) e parity (informazioni di parit√† per il recupero dei dati).
3. **Controllo di Coerenza**: Meccanismi per verificare e correggere l'integrit√† dei dati.
4. **Backup Regolari**: Esecuzione di backup regolari per garantire il recupero dei dati in caso di perdita.

Questi argomenti verranno trattati in dettaglio nella terza parte del libro sulla persistenza, dove esploreremo dispositivi e I/O in generale, e successivamente dischi, RAID e file system in modo approfondito.

## Hello Multithreading

All'inizio del programma, viene dichiarato un array di tipo `pthread_t` chiamato `threads`, la cui dimensione √® determinata dalla costante `NUM_THREADS`. Questo array √® utilizzato per memorizzare gli identificatori dei thread che verranno creati. Inoltre, vengono dichiarate due variabili: `rc`, che verr√† utilizzata per memorizzare il codice di ritorno della funzione `pthread_create`, e `t`, che funger√† da contatore nel ciclo for.

Il ciclo for itera da 0 fino a `NUM_THREADS - 1`. Ad ogni iterazione, viene visualizzato un messaggio che indica la creazione di un nuovo thread, identificato dal valore corrente di `t`. Successivamente, viene chiamata la funzione `pthread_create` per effettivamente creare il thread. Questa funzione prende quattro parametri: un puntatore all'elemento corrente dell'array `threads` (che memorizzer√† l'identificatore del nuovo thread), un puntatore a `NULL` che specifica gli attributi predefiniti del thread, il nome della funzione che il thread eseguir√† (`PrintHello` in questo caso), e un puntatore a `void` che passa un argomento alla funzione del thread. L'argomento passato √® il valore corrente di `t`, castato a `(void *)` per soddisfare la firma della funzione `pthread_create`.

Se la chiamata a `pthread_create` fallisce, restituir√† un valore diverso da 0, che viene assegnato a `rc`. Il codice controlla questo caso e, se si verifica un errore, stampa un messaggio di errore con il codice di ritorno e termina il programma chiamando `exit(-1)`.

Dopo aver creato tutti i thread, il programma chiama `pthread_exit(NULL)`. Questa funzione termina il thread principale, ma lascia che gli altri thread continuino ad eseguire fino al loro completamento. √à un modo per assicurarsi che il programma principale non termini prematuramente, terminando di conseguenza tutti i thread figli prima che abbiano la possibilit√† di completare la loro esecuzione.

In sintesi, questo codice dimostra un modello base per la creazione e gestione di thread multipli in un programma C, utilizzando la libreria Pthreads.

# PrintHello

La funzione `PrintHello` √® definita per essere utilizzata come funzione di esecuzione di un thread in un programma che utilizza la libreria Pthreads. Questa funzione dimostra un semplice esempio di come un thread pu√≤ eseguire un compito specifico, in questo caso, stampare un messaggio sulla console.

Il parametro `void *threadid` √® un puntatore generico che permette di passare qualsiasi tipo di dato alla funzione. Questo √® utile in contesti multithreading dove si desidera passare uno o pi√π valori a un thread quando viene creato. Nell'esempio corrente, si presume che questo puntatore venga utilizzato per passare l'identificatore univoco del thread, anche se, in teoria, potrebbe essere utilizzato per passare qualsiasi tipo di dato.

All'interno della funzione, il primo passo √® convertire il puntatore `threadid` in un tipo `long`. Questa conversione √® necessaria perch√© il puntatore viene passato come un tipo generico (`void *`), ma per stampare l'identificatore del thread si desidera utilizzare un tipo specifico che sia adatto alla funzione `printf`. La conversione esplicita a `(long)` assicura che il valore possa essere stampato correttamente utilizzando il segnaposto `%ld` nella stringa di formato di `printf`.

La funzione `printf` viene quindi utilizzata per stampare un messaggio che include l'identificatore del thread. Questo dimostra come i thread possano eseguire operazioni di input/output e interagire con l'utente o con altre parti del programma.

Infine, la funzione chiama `pthread_exit(NULL)`. Questa chiamata termina esplicitamente il thread corrente, permettendo al thread di uscire pulitamente. Passando `NULL` come parametro, si indica che il thread termina senza restituire un valore specifico. Questo √® un modo comune per gestire la terminazione dei thread in programmi che utilizzano Pthreads, assicurando che le risorse del thread vengano rilasciate correttamente e che il thread possa essere terminato senza influenzare l'esecuzione degli altri thread o del programma principale.

In sintesi, `PrintHello` √® un esempio di funzione di thread che dimostra le basi dell'esecuzione di thread, della gestione dei parametri e della terminazione pulita in un contesto multithreading con Pthreads.

# Makefile 

CC=gcc
LFLAGS=-pthread

#Nota: versioni precedenti di PThreads utilizzavano
#un flag differente ("-lpthread").
#LFLAGS=-lpthread

all: helloMT

helloMT: helloMT.c
	$(CC) $(LFLAGS) -o helloMT helloMT.c

clean:
	rm -f helloMT

Questo codice √® un esempio di un file Makefile utilizzato per compilare un programma C che fa uso di thread, specificamente con la libreria POSIX Threads (Pthreads). Ecco una spiegazione dettagliata di ciascuna parte:

- `CC=gcc`: Questa riga imposta la variabile `CC` sul compilatore GCC (GNU Compiler Collection), che verr√† utilizzato per compilare il programma.

- `LFLAGS=-pthread`: Imposta la variabile `LFLAGS` con l'opzione `-pthread`. Questa opzione dice al compilatore di aggiungere il supporto per la libreria Pthreads. √à importante notare che questa √® l'opzione consigliata per abilitare il supporto ai thread in programmi che utilizzano Pthreads con compilatori moderni.

- `#LFLAGS=-lpthread`: Questa riga √® commentata e mostra un'alternativa pi√π vecchia per abilitare il supporto ai thread, utilizzando il flag `-lpthread`. In passato, questa era la modalit√† standard per collegare la libreria Pthreads, ma nelle versioni pi√π recenti di GCC, `-pthread` √® preferito sia per la compilazione che per il linking.

- `all: helloMT`: Definisce un target chiamato `all`, che dipende dal target `helloMT`. Questo √® un pattern comune nei Makefile, dove `all` √® spesso il target predefinito che costruisce tutto ci√≤ che √® necessario per il progetto.

- `helloMT: helloMT.c`: Questa riga definisce il target `helloMT`, che dipende dal file sorgente [`helloMT.c`]. 
  La regola specifica che per costruire `helloMT`, il Makefile deve eseguire il comando specificato nella riga successiva.

- `$(CC) $(LFLAGS) -o helloMT helloMT.c`: Questo comando utilizza il compilatore specificato nella variabile `CC` (gcc), applica le opzioni di linking specificate in `LFLAGS` (`-pthread`), e compila il file [`helloMT.c`] in un eseguibile chiamato `helloMT`.

- `clean:`: Definisce un target chiamato `clean`, che non dipende da nessun file. Questo target √® comunemente utilizzato per rimuovere i file generati durante la compilazione, per "pulire" la directory di lavoro.

- `rm -f helloMT`: Il comando eseguito dal target `clean`, che rimuove l'eseguibile `helloMT`, utilizzando `rm` (remove) con l'opzione `-f` per forzare la rimozione e ignorare eventuali errori se il file non esiste.

In sintesi, questo Makefile fornisce un metodo semplice per compilare e pulire un programma C che utilizza Pthreads, mostrando anche un esempio di come le opzioni di compilazione e linking possono cambiare nel tempo.

# Compilare un programma utilizzando un Makefile,

Questi passaggi nel terminale:

1. **Apri il Terminale**: Apri un terminale nel sistema operativo che stai utilizzando. Se sei su Linux o macOS, puoi aprire il terminale direttamente. Su Windows, potresti voler utilizzare il Prompt dei comandi (CMD) o PowerShell, oppure, se hai installato un sottosistema Linux per Windows (WSL), puoi usare anche quello.

2. **Naviga alla Directory del Progetto**: Utilizza il comando `cd` (change directory) per navigare alla directory che contiene il tuo Makefile e il codice sorgente. Ad esempio:
   ```bash
   cd /path/to/your/project
   ```
   Sostituisci `/path/to/your/project` con il percorso effettivo della tua directory del progetto.

3. **Esegui Make**: Una volta nella directory corretta, esegui il comando `make` per avviare il processo di compilazione definito nel tuo Makefile. Se il tuo Makefile √® configurato correttamente, questo comando compiler√† il tuo programma seguendo le istruzioni specificate. Per esempio:
   ```bash
   make
   ```
   Questo comando cercher√† un file chiamato `Makefile` nella directory corrente e eseguir√† il primo target definito in esso, che di solito √® il target `all`.
   Per avviare `make` utilizzando un Makefile con un nome specifico diverso da `Makefile`, puoi utilizzare l'opzione `-f` seguita dal nome del file. Se il tuo Makefile si chiama `helloMT.make`, il comando da eseguire nel terminale sar√†:

```bash
make -f helloMT.make
```

1. **Esegui il Programma Compilato**: Se la compilazione √® stata completata con successo, dovresti ora avere un eseguibile nel tuo progetto. Puoi eseguirlo direttamente dal terminale. Ad esempio, se il tuo eseguibile si chiama `helloMT`, puoi eseguirlo con:
   ```bash
   ./helloMT
   ```

2. **Pulizia (Opzionale)**: Se il tuo Makefile include un target `clean` per rimuovere i file generati durante la compilazione, puoi eseguirlo con:
   ```bash
   make clean
   ```
   Questo passaggio √® utile per rimuovere gli eseguibili e altri file intermedi prima di una nuova compilazione o per mantenere pulita la directory del progetto.

Ricorda che i dettagli specifici su come compilare il tuo programma dipenderanno dalle istruzioni definite nel tuo Makefile.

## cosa fa effettivamente un sistema operativo

Ora hai un'idea di cosa fa effettivamente un sistema operativo: prende risorse fisiche, come CPU, memoria o disco, e le virtualizza. Gestisce questioni difficili e complicate relative alla concorrenza. E memorizza i file in modo persistente, rendendoli sicuri a lungo termine. Dato che vogliamo costruire un tale sistema, dobbiamo avere in mente alcuni obiettivi per aiutare a focalizzare la nostra progettazione e implementazione e fare compromessi quando necessario; trovare il giusto insieme di compromessi √® la chiave per costruire sistemi.

Uno degli obiettivi pi√π basilari √® costruire delle astrazioni per rendere il sistema conveniente e facile da usare. Le astrazioni sono fondamentali per tutto ci√≤ che facciamo in informatica. L'astrazione rende possibile scrivere un programma grande dividendolo in pezzi piccoli e comprensibili, scrivere un tale programma in un linguaggio di alto livello come il C senza pensare all'assembly, scrivere codice in assembly senza pensare ai circuiti logici, e costruire un processore partendo dai circuiti logici senza pensare troppo ai transistor. L'astrazione √® cos√¨ fondamentale che a volte dimentichiamo la sua importanza, ma non lo faremo qui; quindi, in ogni sezione, discuteremo alcune delle principali astrazioni che si sono sviluppate nel tempo, dandoti un modo per pensare ai pezzi del sistema operativo.

Un obiettivo nella progettazione e implementazione di un sistema operativo √® fornire alte prestazioni; un altro modo di dire questo √® che il nostro obiettivo √® minimizzare gli overhead del sistema operativo.
La virtualizzazione e rendere il sistema facile da usare sono obiettivi molto importanti, ma non a qualsiasi costo; quindi, dobbiamo sforzarci di fornire la virtualizzazione e altre funzionalit√† del sistema operativo senza eccessivi overhead. Questi overhead si manifestano in diverse forme: tempo extra (pi√π istruzioni) e spazio extra (in memoria o su disco). Cercheremo soluzioni che minimizzino l'uno o l'altro, o entrambi, se possibile. Tuttavia, la perfezione non √® sempre raggiungibile, qualcosa che impareremo a riconoscere e (dove appropriato) tollerare.

Un altro obiettivo sar√† fornire protezione tra le applicazioni, cos√¨ come tra il sistema operativo e le applicazioni. Poich√© desideriamo consentire a molti programmi di funzionare contemporaneamente, vogliamo assicurarci che il comportamento dannoso o accidentale di uno non danneggi gli altri; certamente non vogliamo che un'applicazione sia in grado di danneggiare il sistema operativo stesso (poich√© ci√≤ influenzerebbe tutti i programmi in esecuzione sul sistema). La protezione √® al centro di uno dei principali principi fondamentali di un sistema operativo, che √® quello dell'isolamento; isolare i processi l'uno dall'altro √® la chiave per la protezione e quindi sottende gran parte di ci√≤ che un sistema operativo deve fare.

Il sistema operativo deve anche funzionare senza interruzioni; quando fallisce, tutte le applicazioni in esecuzione sul sistema falliscono anch'esse. A causa di questa dipendenza, i sistemi operativi spesso cercano di fornire un alto grado di affidabilit√†. Poich√© i sistemi operativi diventano sempre pi√π complessi (a volte contenenti milioni di linee di codice), costruire un sistema operativo affidabile √® una grande sfida ‚Äî e infatti, gran parte della ricerca in corso nel campo (incluso parte del nostro lavoro [BS+09, SS+10]) si concentra esattamente su questo problema.

Altri obiettivi hanno senso: l'efficienza energetica √® importante nel nostro mondo sempre pi√π ecologico; la sicurezza (un'estensione della protezione, in realt√†) contro le applicazioni dannose √® fondamentale, soprattutto in questi tempi altamente connessi; la mobilit√† √® sempre pi√π importante poich√© i sistemi operativi vengono eseguiti su dispositivi sempre pi√π piccoli. A seconda di come viene utilizzato il sistema, il sistema operativo avr√† obiettivi diversi e quindi probabilmente sar√† implementato in modi almeno leggermente diversi. Tuttavia, come vedremo, molti dei principi che presenteremo su come costruire un sistema operativo sono utili su una gamma di dispositivi diversi.

## COMPITI 
La maggior parte (e alla fine, tutti) i capitoli di questo libro hanno sezioni di compiti a fine capitolo. Fare questi compiti √® importante, poich√© ciascuno consente a te, lettore, di acquisire maggiore esperienza con i concetti presentati nel capitolo.

Ci sono due tipi di compiti. Il primo √® basato sulla simulazione. Una simulazione di un sistema informatico √® solo un semplice programma che finge di fare alcune delle parti interessanti di ci√≤ che fa un sistema reale, e poi riporta alcune metriche di output per mostrare come si comporta il sistema. Ad esempio, un simulatore di disco rigido potrebbe prendere una serie di richieste, simulare quanto tempo impiegherebbero per essere elaborate da un disco rigido con determinate caratteristiche di prestazioni e poi riportare la latenza media delle richieste.

La cosa interessante delle simulazioni √® che ti permettono di esplorare facilmente come si comportano i sistemi senza la difficolt√† di far funzionare un sistema reale. In effetti, ti permettono persino di creare sistemi che non possono esistere nel mondo reale (ad esempio, un disco rigido con prestazioni incredibilmente veloci) e quindi vedere l'impatto potenziale delle tecnologie future.

Ovviamente, le simulazioni non sono prive di svantaggi. Per loro stessa natura, le simulazioni sono solo approssimazioni di come si comporta un sistema reale. Se un aspetto importante del comportamento del mondo reale viene omesso, la simulazione riporter√† risultati errati. Pertanto, i risultati di una simulazione dovrebbero sempre essere trattati con un po' di sospetto. Alla fine, ci√≤ che conta √® come si comporta un sistema nel mondo reale.

Il secondo tipo di compito richiede l'interazione con codice reale. Alcuni di questi compiti sono focalizzati sulla misurazione, mentre altri richiedono solo un piccolo sviluppo e sperimentazione su piccola scala. Entrambi sono solo piccole incursioni nel mondo pi√π grande in cui dovresti entrare, ovvero come scrivere codice di sistema in C su sistemi basati su UNIX. In effetti, sono necessari progetti su larga scala, che vanno oltre questi compiti, per spingerti in questa direzione; quindi, oltre a fare i compiti, ti raccomandiamo fortemente di fare progetti per consolidare le tue abilit√† di sistema. Consulta questa pagina (https://github.com/remzi-arpacidusseau/ostep-projects) per alcuni progetti.

Per fare questi compiti, probabilmente dovrai essere su una macchina basata su UNIX, eseguendo Linux, macOS o un sistema simile. Dovrebbe anche avere un compilatore C installato (ad esempio, gcc) cos√¨ come Python. Dovresti anche sapere come modificare il codice in un vero editor di codice di qualche tipo.

# LETTORI-SCRITTORI

Il problema di sincronizzare lettori e scrittori in C √® un classico problema di concorrenza che si verifica quando pi√π processi (lettori e scrittori) accedono contemporaneamente a una risorsa condivisa (come una variabile, una struttura dati, un file, ecc.). L'obiettivo √® garantire la coerenza dei dati e prevenire condizioni di race e inconsistenze nel caso in cui un lettore e uno scrittore tentino di accedere alla risorsa simultaneamente.

Ecco una panoramica del problema di sincronizzazione tra lettori e scrittori in C:

- **Lettori**:
  - I lettori possono accedere alla risorsa condivisa contemporaneamente per la lettura.
  - Pi√π lettori possono leggere i dati contemporaneamente senza causare problemi di coerenza.
  - Tuttavia, i lettori non devono leggere i dati mentre un processo scrittore sta scrivendo sulla risorsa.

- **Scrittori**:
  - Gli scrittori richiedono un accesso esclusivo alla risorsa condivisa per la scrittura.
  - Un processo scrittore deve essere l'unico ad accedere alla risorsa durante la scrittura e deve bloccare l'accesso ai lettori durante questo periodo.
  - Gli scrittori devono attendere fino a quando tutti i lettori attivi non hanno completato la lettura prima di poter scrivere.

Il problema di sincronizzare lettori e scrittori in C richiede una corretta gestione della concorrenza per garantire che i lettori e gli scrittori accedano alla risorsa condivisa in modo sicuro ed efficiente, evitando condizioni di `race, deadlock e starvation` (concetti fondamentali nella programmazione concorrente e nella gestione delle risorse condivise).

Per risolvere questo problema, sono necessari meccanismi di sincronizzazione come `semafori, mutex, variabili di condizione` o altre tecniche per coordinare l'accesso dei lettori e degli scrittori alla risorsa condivisa in modo coerente e robusto.

# Condizione di Race:

- Una condizione di race si verifica quando il comportamento di un programma dipende dall'ordine di esecuzione delle istruzioni, che pu√≤ variare a seconda di come vengono schedulate le attivit√† da parte del sistema operativo.
- In una condizione di race, due o pi√π processi concorrenti possono accedere e modificare una risorsa condivisa in modo non deterministico, portando a risultati imprevedibili o incorretti.
- Le condizioni di race possono causare problemi come letture inconsistenti, scritture sovrascritte, valori errati o comportamenti non definiti nel programma.
  
Modificare una risorsa condivisa in modo non deterministico cosa significa in dettaglio:

- **Non deterministico**:
  - In informatica, un'operazione o un processo √® deterministico se produce sempre lo stesso risultato quando viene eseguito con le stesse condizioni iniziali.
  
  - Al contrario, un'operazione non deterministica √® un'operazione la cui esecuzione pu√≤ portare a risultati diversi in base a fattori esterni o casuali.
  
- **Modifica di una risorsa condivisa**:
  - Quando pi√π processi concorrenti modificano una risorsa condivisa, ad esempio una variabile o una struttura dati, senza una corretta sincronizzazione, possono verificarsi situazioni in cui le modifiche non avvengono in modo prevedibile.
  
- **Esempio `helloRace.c`**:
  - Supponiamo che due processi stiano leggendo e aggiornando il valore di una variabile condivisa. Se entrambi i processi leggono il valore corrente contemporaneamente e tentano di aggiornarlo senza una sincronizzazione adeguata, potrebbe verificarsi una condizione di race.
  - In questo scenario, il risultato finale dipender√† dall'ordine in cui vengono eseguite le operaziprintf("oni di lettura e scrittura da parte dei due processi, portando a un comportamento non deterministico e imprevedibile.

In sintesi, modificare una risorsa condivisa in modo non deterministico significa che il risultato delle operazioni di accesso e modifica della risorsa non pu√≤ essere previsto con certezza a causa della concorrenza e della mancanza di sincronizzazione tra i processi che accedono ad essa. Questo pu√≤ portare a problemi di coerenza dei dati, letture inconsistente e comportamenti imprevisti nel programma.

Un esempio classico di condizione di race (race condition) si verifica quando due o pi√π thread accedono in modo concorrente a una risorsa condivisa e almeno uno di questi accessi √® una scrittura. Senza una sincronizzazione adeguata, l'ordine degli accessi pu√≤ portare a risultati inaspettati e difficili da prevedere.

Ecco un semplice esempio in C che dimostra una condizione di race. Il programma crea due thread che incrementano concorrentemente una variabile condivisa senza meccanismi di sincronizzazione come mutex o semafori.

**Passaggi:**

1. Includere i file di intestazione necessari.
2. Definire una variabile globale condivisa.
3. Definire la funzione che sar√† eseguita dai thread, la quale incrementa la variabile condivisa.
4. Nel `main`, creare i thread e attendere che terminino.
5. Stampare il valore finale della variabile condivisa.

**Codice:**

```c
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>

int counter = 0; // Variabile condivisa

void *incrementCounter(void *threadid) {
    int i;
    for(i = 0; i < 100000; i++) {
        counter++; // Incrementa la variabile condivisa
    }
    pthread_exit(NULL);
}

int main() {
    pthread_t threads[2];
    int rc;
    long t;

    for(t = 0; t < 2; t++) {
        printf("Creazione del thread %ld\n", t);
        rc = pthread_create(&threads[t], NULL, incrementCounter, (void *)t);
        if (rc) {
            printf("ERRORE; codice di ritorno da pthread_create() √® %d\n", rc);
            exit(-1);
        }
    }

    // Attendere che i thread abbiano terminato
    for(t = 0; t < 2; t++) {
        pthread_join(threads[t], NULL);
    }

    printf("Valore finale di counter = %d\n", counter);
    pthread_exit(NULL);
}
```

# Soluzioni alla Condizione di Race:

I semafori e i mutex sono entrambi strumenti utilizzati per la gestione della concorrenza e la sincronizzazione tra processi o thread in un ambiente multitasking. Tuttavia, presentano alcune differenze chiave nel loro funzionamento e nell'utilizzo.

- **Mutex (Mutual Exclusion)**:
  - **Scopo**: I mutex vengono utilizzati per garantire l'accesso esclusivo a una risorsa condivisa tra diversi processi o thread.
  - **Stato**: Possono essere in uno dei due stati: bloccato o sbloccato. Un mutex bloccato indica che la risorsa √® in uso, impedendo ad altri processi o thread di accedervi finch√© non viene rilasciato.
  - **Utilizzo**: Un mutex √® progettato per essere posseduto da un solo processo o thread alla volta.
  - **Semplicit√†**: I mutex sono pi√π leggeri e pi√π efficienti dei semafori quando si tratta di garantire l'accesso esclusivo a una risorsa.

- **Semafori**:
  - **Scopo**: I semafori possono essere utilizzati per sincronizzare l'accesso a risorse condivise e per gestire la cooperazione tra processi o thread.
  - **Valore**: Un semaforo pu√≤ avere un valore intero non negativo e viene utilizzato per controllare l'accesso concorrente a una risorsa.
  - **Operazioni**: Le operazioni principali sui semafori includono `wait` (P) e `signal` (V) per bloccare e sbloccare il semaforo rispettivamente.
  - **Pi√π flessibilit√†**: I semafori offrono una maggiore flessibilit√† rispetto ai mutex, consentendo di implementare diverse politiche di sincronizzazione.

- **Differenze**:
  - I mutex sono pi√π adatti per garantire l'accesso esclusivo a una risorsa, mentre i semafori sono pi√π flessibili e possono essere utilizzati per implementare diverse strategie di sincronizzazione.
  - I mutex sono progettati per il possesso esclusivo da parte di un singolo processo o thread, mentre i semafori possono essere acquisiti e rilasciati da pi√π processi contemporaneamente.
  - I mutex sono tipicamente pi√π leggeri e pi√π efficienti rispetto ai semafori, ma offrono una funzionalit√† pi√π limitata.

In sintesi, mentre i mutex sono ideali per garantire l'accesso esclusivo a una risorsa condivisa, i semafori offrono una maggiore flessibilit√† e possono essere utilizzati per una variet√† di scenari di sincronizzazione e cooperazione tra processi o thread. La scelta tra mutex e semafori dipende dalle esigenze specifiche dell'applicazione e dal tipo di sincronizzazione richiesta.

## Usiamo i Mutex

Per risolvere il problema della race condition nel codice, possiamo utilizzare un mutex per garantire l'accesso esclusivo alla variabile condivisa `counter` durante la sua modifica. Ecco i passaggi per implementare questa soluzione:

1. Inizializzare un mutex globalmente.
2. Bloccare il mutex prima di incrementare la variabile `counter` nella funzione `incrementCounter`.
3. Sbloccare il mutex dopo aver incrementato la variabile `counter`.
4. Distruggere il mutex nella funzione `main` dopo aver atteso la terminazione di tutti i thread.

Ecco il codice modificato:

```c
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>

int counter = 0; // Variabile condivisa
pthread_mutex_t lock; // Mutex per controllare l'accesso a `counter`

void *incrementCounter(void *threadid) {
    int i;
    for(i = 0; i < 100000; i++) {
        pthread_mutex_lock(&lock); // Blocca il mutex
        counter++; // Incrementa la variabile condivisa
        pthread_mutex_unlock(&lock); // Sblocca il mutex
    }
    pthread_exit(NULL);
}

int main() {
    pthread_t threads[2];
    int rc;
    long t;

    // Inizializza il mutex
    if (pthread_mutex_init(&lock, NULL) != 0) {
        printf("Inizializzazione del mutex fallita\n");
        return 1;
    }

    for(t = 0; t < 2; t++) {
        printf("Creazione del thread %ld\n", t);
        rc = pthread_create(&threads[t], NULL, incrementCounter, (void *)t);
        if (rc) {
            printf("ERRORE; codice di ritorno da pthread_create() √® %d\n", rc);
            exit(-1);
        }
    }

    // Attendere che i thread abbiano terminato
    for(t = 0; t < 2; t++) {
        pthread_join(threads[t], NULL);
    }

    printf("Valore finale di counter = %d\n", counter);

    // Distruggi il mutex
    pthread_mutex_destroy(&lock);

    pthread_exit(NULL);
}
```

Con queste modifiche, l'accesso alla variabile `counter` √® sincronizzato tra i thread, prevenendo cos√¨ la race condition e garantendo che il valore finale di `counter` sia corretto dopo l'esecuzione di tutti i thread.

## Uso dei smefori

Per risolvere problemi di sincronizzazione tra thread, come la condizione di race, i semafori possono essere utilizzati efficacemente. Un semaforo √® un tipo di variabile che viene utilizzata per controllare l'accesso a una risorsa comune da parte di pi√π thread in un ambiente di programmazione concorrente.

Ecco un esempio di come utilizzare un semaforo in C per risolvere una condizione di race incrementando una variabile condivisa. Questo esempio utilizza la libreria POSIX Threads (Pthreads) e i semafori POSIX.

**Passaggi:**

1. Includere i file di intestazione necessari.
2. Inizializzare un semaforo.
3. Definire una variabile globale condivisa.
4. Definire la funzione che sar√† eseguita dai thread, la quale incrementa la variabile condivisa utilizzando il semaforo per sincronizzare l'accesso.
5. Nel `main`, inizializzare il semaforo, creare i thread, attendere che terminino, e poi distruggere il semaforo.
6. Stampare il valore finale della variabile condivisa.

**Codice:**

```c
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>
#include <semaphore.h>

int counter = 0; // Variabile condivisa
sem_t sem; // Semaforo

void *incrementCounter(void *threadid) {
    int i;
    for(i = 0; i < 100000; i++) {
        sem_wait(&sem); // Decrementa il semaforo (blocca se √® 0)
        counter++; // Incrementa la variabile condivisa
        sem_post(&sem); // Incrementa il semaforo (sblocca un altro thread)
    }
    pthread_exit(NULL);
}

int main() {
    pthread_t threads[2];
    int rc;
    long t;

    // Inizializza il semaforo
    sem_init(&sem, 0, 1); // 0 per semaforo tra thread, 1 √® il valore iniziale

    for(t = 0; t < 2; t++) {
        printf("Creazione del thread %ld\n", t);
        rc = pthread_create(&threads[t], NULL, incrementCounter, (void *)t);
        if (rc) {
            printf("ERRORE; codice di ritorno da pthread_create() √® %d\n", rc);
            exit(-1);
        }
    }

    // Attendere che i thread abbiano terminato
    for(t = 0; t < 2; t++) {
        pthread_join(threads[t], NULL);
    }

    printf("Valore finale di counter = %d\n", counter);

    // Distruggi il semaforo
    sem_destroy(&sem);

    pthread_exit(NULL);
}
```

**Note:**

- `sem_init` inizializza il semaforo. Il secondo parametro `0` indica che il semaforo √® condiviso tra i thread dello stesso processo. Il terzo parametro √® il valore iniziale del semaforo.
- `sem_wait` decrementa (blocca) il semaforo. Se il valore del semaforo √® zero, il thread chiama `sem_wait` verr√† bloccato fino a quando il semaforo non verr√† incrementato.
- `sem_post` incrementa (sblocca) il semaforo, permettendo ad altri thread bloccati di procedere.
- `sem_destroy` distrugge il semaforo una volta che non √® pi√π necessario.

Questo approccio garantisce che solo un thread per volta possa incrementare la variabile `counter`, eliminando la condizione di race.

I semafori POSIX sono un tipo di semaforo utilizzato nei sistemi operativi basati su standard POSIX (Portable Operating System Interface). I semafori POSIX forniscono un meccanismo per la sincronizzazione e la gestione della concorrenza tra processi o thread all'interno di un sistema operativo. Ecco alcune caratteristiche e concetti chiave relativi ai semafori POSIX:

# Semafori POSIX

- I semafori POSIX sono associati a un nome univoco nel sistema operativo, consentendo a processi o thread diversi di accedere allo stesso semaforo per la sincronizzazione.
  
- Possono essere utilizzati per la gestione di risorse condivise, la prevenzione delle race condition e la sincronizzazione tra processi che cooperano tra loro.

## Operazioni sui semafori

Le operazioni principali che possono essere eseguite sui semafori POSIX includono:
    - `sem_open`: Per creare o aprire un semaforo con un nome specifico.
    - `sem_wait` (o `sem_wait`): Per bloccare il processo o il thread fino a quando il semaforo non diventa - **disponibile.
    - `sem_post` (o `sem_signal`): Per sbloccare il semaforo dopo che √® stato utilizzato.
    - `sem_close`: Per chiudere il semaforo dopo averlo utilizzato.
    - `sem_unlink`: Per rimuovere il semaforo dal sistema.

## Tipi di semafori

I semafori POSIX possono essere di due tipi principali:

- **Semafori non nominati**: Sono associati a variabili di tipo `sem_t` e sono utilizzati localmente all'interno di un processo.
  
La variabile `sem_t` √® un tipo di dato definito dalla libreria POSIX Semaphores. Viene utilizzata per rappresentare un semaforo in un programma C. Un semaforo √® un meccanismo di sincronizzazione che pu√≤ essere utilizzato per controllare l'accesso a una risorsa condivisa da parte di pi√π thread o processi in un ambiente di programmazione concorrente.

Nel contesto del frammento di codice fornito, `sem_t sem;` dichiara una variabile di tipo `sem_t`, che viene utilizzata per sincronizzare l'accesso alla variabile condivisa `counter` tra i thread. Questo √® particolarmente utile per prevenire condizioni di race, dove l'ordine non deterministico degli accessi alla risorsa condivisa pu√≤ portare a risultati imprevedibili o errati.

**Funzioni principali associate a `sem_t`

- **`sem_init(sem_t *sem, int pshared, unsigned int value)`:** Inizializza il semaforo puntato da `sem` con un valore iniziale specificato da `value`. Il parametro `pshared` determina se il semaforo deve essere condiviso tra i thread di un processo (`pshared = 0`) o tra processi (`pshared != 0`).

- **`sem_wait(sem_t *sem)`:** Decrementa (blocca) il semaforo puntato da [`sem`]. Se il valore del semaforo √® maggiore di zero, il valore viene decrementato e la funzione ritorna immediatamente.

Se il valore √® zero, il thread chiama `sem_wait` viene bloccato fino a quando il semaforo non viene incrementato da un altro thread o processo.

- **`sem_post(sem_t *sem)`:** Incrementa (sblocca) il semaforo puntato da `sem`. Se altri thread sono bloccati in attesa del semaforo, sblocca uno di questi thread.

- **`sem_destroy(sem_t *sem)`:** Distrugge il semaforo puntato da `sem`, liberando le risorse associate. Dopo la chiamata a `sem_destroy`, il semaforo non pu√≤ pi√π essere utilizzato fino a una successiva inizializzazione.

L'uso di `sem_t` e delle relative funzioni permette di implementare sezioni critiche nel codice, garantendo che solo un thread per volta possa eseguire determinate operazioni su risorse condivise, prevenendo cos√¨ condizioni di race e garantendo la correttezza del programma in ambienti concorrenti.

- **Semafori nominati**: Sono associati a un nome nel sistema operativo e possono essere utilizzati per la sincronizzazione tra processi diversi.

- **Gestione degli errori**:
  - Le chiamate di sistema che coinvolgono i semafori POSIX restituiscono valori che indicano lo stato di operazioni come la creazione, l'apertura, il blocco e lo sblocco dei semafori. √à importante gestire correttamente questi valori di ritorno per la gestione degli errori.

I semafori POSIX sono uno strumento potente per la gestione della concorrenza nei sistemi operativi e consentono ai programmatori di sviluppare applicazioni concorrenti sicure e efficienti. La corretta gestione dei semafori POSIX pu√≤ aiutare a prevenire condizioni di race, deadlock e starvation, migliorando la robustezza e le prestazioni del software concorrente.

# Condizione di Deadlock:

- Un deadlock si verifica quando due o pi√π processi rimangono bloccati in attesa di risorse che sono detenute da altri processi nello stesso insieme di processi.
- In un deadlock, nessun processo pu√≤ procedere perch√© ciascuno attende una risorsa che √® detenuta da un altro processo, creando un'impasse nella concorrenza.
- Per risolvere un deadlock, √® necessario rilasciare le risorse bloccate o implementare algoritmi di rilevamento e risoluzione dei deadlock.

## Esempio di condizione di deadlock

Per illustrare un esempio di condizione di deadlock, useremo due thread e due mutex. Ogni thread cercher√† di acquisire entrambi i mutex, ma in ordine inverso, creando una situazione in cui ciascun thread detiene un mutex e attende l'altro, portando a un deadlock.

Ecco i passaggi in pseudocodice:

1. Inizializzare due mutex, `mutex1` e `mutex2`.
2. Creare due funzioni, `threadFunction1` e `threadFunction2`. Ogni funzione cercher√† di acquisire entrambi i mutex, ma in ordine inverso:
   - `threadFunction1` cercher√† di acquisire prima `mutex1` e poi `mutex2`.
   - `threadFunction2` cercher√† di acquisire prima `mutex2` e poi `mutex1`.
3. Creare e avviare due thread, uno per `threadFunction1` e uno per `threadFunction2`.
4. Attendere che entrambi i thread terminino (questo non accadr√† a causa del deadlock).

Ecco come potrebbe essere implementato in C:

```c
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>

pthread_mutex_t mutex1 = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t mutex2 = PTHREAD_MUTEX_INITIALIZER;

// Funzione eseguita dal primo thread
void* threadFunction1(void* arg) {
    pthread_mutex_lock(&mutex1);
    printf("Thread 1 ha acquisito mutex1\n");
    sleep(1); // Simula il lavoro e d√† tempo al thread 2 di bloccare mutex2
    pthread_mutex_lock(&mutex2);
    printf("Thread 1 ha acquisito mutex2\n");
    pthread_mutex_unlock(&mutex2);
    pthread_mutex_unlock(&mutex1);
    return NULL;
}

// Funzione eseguita dal secondo thread
void* threadFunction2(void* arg) {
    pthread_mutex_lock(&mutex2);
    printf("Thread 2 ha acquisito mutex2\n");
    sleep(1); // Simula il lavoro e d√† tempo al thread 1 di bloccare mutex1
    pthread_mutex_lock(&mutex1);
    printf("Thread 2 ha acquisito mutex1\n");
    pthread_mutex_unlock(&mutex1);
    pthread_mutex_unlock(&mutex2);
    return NULL;
}

int main() {
    pthread_t thread1, thread2;

    pthread_create(&thread1, NULL, threadFunction1, NULL);
    pthread_create(&thread2, NULL, threadFunction2, NULL);

    pthread_join(thread1, NULL);
    pthread_join(thread2, NULL);

    return 0;
}
```

In questo esempio, entrambi i thread rimarranno bloccati indefinitamente cercando di acquisire il mutex che l'altro thread ha gi√† bloccato, risultando in un deadlock.

## Evitare la condizione di deadlock

Per risolvere la condizione di deadlock nel codice fornito, possiamo adottare l'approccio di acquisire i mutex sempre nello stesso ordine in entrambe le funzioni dei thread. Questo evita la situazione in cui un thread aspetta l'altro per rilasciare un mutex che ha gi√† acquisito. Modificheremo entrambe le funzioni dei thread per acquisire prima `mutex1` e poi `mutex2`.


```c
// Funzione eseguita dal secondo thread
void* threadFunction2(void* arg) {
    pthread_mutex_lock(&mutex1); // Modifica qui: acquisisce prima mutex1
    printf("Thread 2 ha acquisito mutex1\n");
    sleep(1); // Simula il lavoro e d√† tempo al thread 1 di bloccare mutex2
      pthread_mutex_lock(&mutex2); // Poi acquisisce mutex2
        printf("Thread 2 ha acquisito mutex2\n");
      pthread_mutex_unlock(&mutex2);
    pthread_mutex_unlock(&mutex1);
    return NULL;
}
```

Con questa modifica, entrambi i thread cercheranno di acquisire i mutex nello stesso ordine (`mutex1` e poi `mutex2`), eliminando cos√¨ la possibilit√† di un deadlock. Questo approccio √® uno dei modi pi√π semplici ed efficaci per prevenire i deadlock quando si lavora con pi√π mutex.

# Starvation

- La starvation si verifica quando un processo non riesce a ottenere l'accesso alle risorse di cui ha bisogno per essere eseguito a causa di priorit√† inferiori rispetto ad altri processi.

- Un processo in stato di starvation pu√≤ essere costantemente ignorato dai meccanismi di schedulazione del sistema operativo, impedendogli di fare progressi o completare il suo lavoro.

- La starvation pu√≤ verificarsi in situazioni in cui i processi con priorit√† pi√π elevate monopolizzano le risorse, impedendo ai processi con priorit√† pi√π basse di essere eseguiti.

Queste condizioni sono importanti da considerare durante lo sviluppo di programmi concorrenti per garantire che il codice sia robusto, efficiente e privo di problemi di concorrenza che potrebbero compromettere la correttezza e le prestazioni del sistema.

La sincronizzazione tra lettori e scrittori nell'accesso a una variabile condivisa utilizzando i Pthreads (pthread.h) in C pu√≤ essere gestita in diversi modi per garantire la correttezza e la coerenza dei dati.
Ecco un approccio comune utilizzando i semafori:

1. **Utilizzo di Semafori**:
   - Creare due semafori, uno per i lettori e uno per gli scrittori.
   - Utilizzare un semaforo mutex per garantire l'accesso esclusivo alla variabile condivisa.
   - Implementare un contatore per tracciare il numero di lettori attivi.
   - Utilizzare un semaforo per garantire l'accesso esclusivo alla variabile contatore dei lettori.
   
2. **Implementazione**:
   - Quando un lettore desidera leggere la variabile condivisa:
     - Acquisire il semaforo per il contatore dei lettori.
     - Incrementare il contatore dei lettori (se √® il primo lettore, acquisire il semaforo mutex).
     - Rilasciare il semaforo per il contatore dei lettori.
     - Leggere la variabile condivisa.
     - Se √® l'ultimo lettore, rilasciare il semaforo mutex.
   
   - Quando uno scrittore desidera scrivere sulla variabile condivisa:
     - Acquisire il semaforo mutex.
     - Bloccare l'accesso ai lettori acquisendo il semaforo per il contatore dei lettori.
     - Scrivere sulla variabile condivisa.
     - Rilasciare il semaforo mutex e il semaforo per il contatore dei lettori.

3. **Considerazioni**:
   - Assicurati di proteggere tutte le operazioni critiche con i semafori per evitare condizioni di race.
   - Gestisci correttamente la priorit√† tra lettori e scrittori per evitare starvation.
   - Testa attentamente il tuo codice per assicurarti che la sincronizzazione funzioni correttamente in tutte le situazioni.

Questo √® solo un esempio di come gestire la sincronizzazione tra lettori e scrittori utilizzando i Pthreads e i semafori in C. √à importante capire le esigenze specifiche del tuo problema e adattare l'implementazione di conseguenza.

# Il prodotto scalare tra due vettori

Il prodotto scalare tra due vettori √® un'operazione matematica che restituisce un singolo valore numerico ottenuto moltiplicando gli elementi corrispondenti dei due vettori e sommando i risultati. In altre parole, il prodotto scalare di due vettori vettore A = [a1, a2, ..., an] e vettore B = [b1, b2, ..., bn] √® dato dalla seguente formula:

**Prodotto Scalare** = a1 * b1 + a2 * b2 + ... + an * bn

Questo calcolo produce un singolo valore che rappresenta il prodotto interno dei due vettori nello spazio euclideo.

Ecco un esempio di un programma in C che calcola il prodotto scalare tra due vettori:

```c
#include <stdio.h>

#define N 3 // Dimensione dei vettori

int main() {
    int vettoreA[N] = {2, 3, 4};
    int vettoreB[N] = {1, 5, 2};

    int prodotto_scalare = 0;

    for (int i = 0; i < N; i++) {
        prodotto_scalare += vettoreA[i] * vettoreB[i];
    }

    printf("Il prodotto scalare tra i due vettori √®: %d\n", prodotto_scalare);

    return 0;
}
```

In questo esempio, abbiamo due vettori `vettoreA` e `vettoreB` di dimensione 3. Il programma calcola il prodotto scalare tra i due vettori utilizzando un ciclo for per moltiplicare gli elementi corrispondenti e sommare i risultati. Alla fine, viene stampato il valore del prodotto scalare calcolato.

Esempio di esecuzione:
```
Il prodotto scalare tra i due vettori √®: 20
```

Questo programma semplice illustra come calcolare il prodotto scalare tra due vettori in C. Ci sono diverse implementazioni possibili a seconda delle esigenze specifiche e del linguaggio di programmazione utilizzato.

Per un esempio di implementazione, [vedi](prodottoScalareMT.c).

Utilizzare i thread al posto dei processi per la programmazione parallela presenta diversi vantaggi, tra cui:

1. **Efficienza**: I thread condividono lo stesso spazio di indirizzamento e le risorse del processo padre, il che li rende pi√π leggeri rispetto ai processi. La creazione di un thread √® generalmente pi√π veloce rispetto alla creazione di un processo.

2. **Velocit√† di comunicazione**: I thread all'interno dello stesso processo possono comunicare tra loro pi√π facilmente e velocemente rispetto ai processi che devono utilizzare meccanismi di comunicazione interprocessuale come le code di messaggi o le pipe.

3. **Condivisione di risorse**: I thread all'interno dello stesso processo condividono lo stesso contesto di memoria, consentendo una condivisione diretta dei dati senza la necessit√† di meccanismi di comunicazione aggiuntivi.

4. **Coordinazione pi√π semplice**: Poich√© i thread condividono lo stesso contesto di esecuzione, la sincronizzazione e la coordinazione tra di essi sono pi√π semplici rispetto alla gestione della concorrenza tra processi separati.

5. **Scalabilit√†**: Utilizzando i thread, √® possibile sfruttare in modo pi√π efficiente i sistemi multi-core e multi-processore, distribuendo compiti tra i diversi thread per sfruttare al massimo la potenza di calcolo disponibile.

Per fornire un esempio pratico, consideriamo un programma in C che utilizza i thread per calcolare la somma di un array di numeri. L'idea √® dividere l'array in parti uguali in base al numero di core disponibili nel sistema, assegnando a ogni thread la responsabilit√† di sommare una parte dell'array. Questo approccio permette di sfruttare la parallelizzazione per accelerare il calcolo.

## Pseudocodice

1. Determinare il numero di core disponibili nel sistema.
2. Dividere l'array in parti uguali in base al numero di core.
3. Creare un thread per ogni parte dell'array.
   - Ogni thread calcola la somma della sua parte dell'array.
4. Attendere che tutti i thread completino.
5. Sommare i risultati parziali ottenuti da ogni thread per ottenere il risultato finale.

[Questo codice](sommaScalabile.c) dimostra come dividere un compito (la somma di un array) tra diversi thread per sfruttare la potenza di calcolo dei multi-core.

Challeng  Crea: prodottoScalareScalabileMT.c

6. **Maggiore flessibilit√†**: I thread consentono una gestione pi√π flessibile delle attivit√† parallele all'interno di un'applicazione, consentendo di creare strutture di controllo pi√π complesse e reattive.

In generale, l'uso dei thread risulta pi√π vantaggioso per la programmazione parallela in situazioni in cui √® necessaria una comunicazione veloce tra le unit√† di elaborazione e la condivisione efficiente delle risorse, mentre l'uso dei processi pu√≤ essere pi√π indicato quando sono richieste maggiori misure di isolamento e protezione tra le attivit√† parallele.

Per un esempio di implementazione con uso di treads e processi , [vedi](prodottoScalareMP.c).

## Problemi coinvolgono la gestione della concorrenza e della sincronizzazione tra thread

Nella programmazione parallela, il **problema del produttore e consumatore** e il **problema di lettura e scrittura** sono entrambi classici problemi di sincronizzazione che coinvolgono la condivisione di risorse tra pi√π thread. Ecco le principali differenze tra i due:

**Problema di Lettura e Scrittura**

- Coinvolge due tipi di operazioni: operazioni di lettura e operazioni di scrittura su una risorsa condivisa, ad esempio un file o una variabile.
- Scopo principale √® garantire che pi√π lettori possano accedere contemporaneamente alla risorsa per la lettura, ma che un solo scrittore possa accedere alla risorsa per la scrittura in modo esclusivo.
- La sincronizzazione avviene per evitare la sovrascrittura dei dati durante la scrittura e garantire la coerenza dei dati durante la lettura.
- Pu√≤ verificarsi il problema della "scrittura sporca" (lettura di dati non validi a causa di scritture in corso) o della "lettura sporca" (lettura di dati non validi a causa di scritture incomplete).

**Problema del Produttore e Consumatore**

- Coinvolge due tipi di thread: i produttori che producono dati e li inseriscono in un buffer e i consumatori che consumano i dati dal buffer.
- Scopo principale √® garantire che i produttori non producano dati quando il buffer √® pieno e che i consumatori non consumino dati quando il buffer √® vuoto.
- La sincronizzazione avviene tramite meccanismi come semafori o mutex per garantire l'accesso sicuro e concorrente al buffer condiviso.
- Pu√≤ verificarsi il problema della sovrapproduzione (produttori che producono pi√π dati di quelli consumati) o della sottoproduzione (consumatori che consumano dati non ancora disponibili).

## Altri classici problemi di concorrenza nella programmazione parallela

1. **Problema dei filosofi a cena**: Rappresenta la sincronizzazione di pi√π processi (filosofi) che condividono risorse (forchette) per evitare situazioni di deadlock e fame. Ogni filosofo deve prendere le forchette a lui adiacenti per mangiare.

2. **Problema del barbiere addormentato**: Simula la situazione in cui un barbiere dorme finch√© non arriva un cliente. Il barbiere deve gestire l'accesso alla sedia del barbiere e alla sala d'attesa in modo sincronizzato per servire i clienti.

Questi problemi sono spesso utilizzati per dimostrare l'efficacia delle tecniche di sincronizzazione e gestione della concorrenza nella programmazione parallela.

## Shared Memory

La memoria condivisa con **shm (Shared Memory)** √® un meccanismo di comunicazione e condivisione di dati tra processi su sistemi Unix-like. Ecco alcune caratteristiche e vantaggi della memoria condivisa con **shm**:

- **Caratteristiche**:
  - Consente a pi√π processi di accedere alla stessa area di memoria condivisa, facilitando la comunicazione e lo scambio di dati tra di essi.
  - 
  - Pi√π efficiente rispetto ad altri metodi di comunicazione interprocessuale come code di messaggi o socket, poich√© elimina la necessit√† di copiare i dati tra i processi.
  - √à spesso utilizzata per la comunicazione e la sincronizzazione tra processi paralleli o concorrenti.

- **Vantaggi**:
  - **Velocit√†**: La memoria condivisa √® pi√π veloce rispetto ad altre forme di comunicazione interprocessuale poich√© i processi condividono direttamente lo spazio di memoria, evitando costose operazioni di copia dei dati.
  - **Semplicit√†**: √à relativamente semplice da implementare e utilizzare, consentendo ai processi di accedere direttamente alla memoria condivisa.
  - **Efficienza**: Riduce il carico sul sistema in quanto non richiede la creazione di canali di comunicazione aggiuntivi come nel caso delle code di messaggi o socket.

Per utilizzare la memoria condivisa con **shm**, i processi devono creare un'area di memoria condivisa condivisa tra di loro e condividerne l'identificatore per potervi accedere. √à importante sincronizzare correttamente l'accesso alla memoria condivisa utilizzando meccanismi come semafori o mutex per evitare problemi di concorrenza e garantire la coerenza dei dati condivisi.

In sintesi, la memoria condivisa con **shm** √® un potente strumento per la comunicazione e la condivisione di dati tra processi su sistemi Unix-like, offrendo velocit√†, semplicit√† ed efficienza nella gestione della concorrenza e della sincronizzazione.

```c
#include <stdio.h> // Inclusione della libreria standard di input/output
#include <stdlib.h> // Inclusione della libreria standard per funzioni di utilit√† generale
#include <unistd.h> // Inclusione della libreria per le chiamate di sistema POSIX
#include <sys/wait.h> // Inclusione della libreria per la gestione delle attese dei processi
#include <sys/shm.h> // Inclusione della libreria per la gestione della memoria condivisa

int main() {
  key_t chiave = ftok(".",'A'); // Generazione di una chiave unica per la memoria condivisa

  // Crea una memoria condivisa in cui contenere un valore intero
  // Il numero di bytes √® pari a sizeof(int)
  int id_shm = shmget(chiave, sizeof(int), IPC_CREAT|0664); // Richiesta di un segmento di memoria condivisa
  if(id_shm<0){
    perror("errore shmget"); // Stampa dell'errore se la creazione fallisce
    exit(1); // Terminazione del programma in caso di errore
  }

  // Si accede alla variabile in memoria condivisa tramite puntatore
  // (in questo caso puntatore a intero "int *")
  int * p = shmat(id_shm, NULL, 0); // Collegamento al segmento di memoria condivisa
  if(p==(void *)-1) {
    perror("errore shmat"); // Stampa dell'errore se il collegamento fallisce
  }

  key_t pid = fork(); // Creazione di un processo figlio

  // Sia il processo padre sia il figlio ottengono una copia
  // del puntatore a memoria condivisa "p"
  if(pid==0) {
    // Processo figlio
    printf("FIGLIO: scrivo 123...\n"); // Stampa di un messaggio dal figlio
    *p = 123; // Scrittura del valore 123 nella memoria condivisa
    sleep(3); // Attesa di 3 secondi
    exit(0); // Terminazione del processo figlio
  } else if(pid>0) {
    // Processo padre
    wait(NULL); // Attesa della terminazione del processo figlio
    printf("PADRE: leggo il valore: %d\n", *p); // Lettura e stampa del valore dalla memoria condivisa
    shmctl(id_shm, IPC_RMID, 0); // Rimozione del segmento di memoria condivisa
 }
}
```

Il codice fornito √® un esempio di come due processi (padre e figlio) possono comunicare attraverso l'uso della memoria condivisa in C su sistemi Unix/Linux. Il programma dimostra la creazione di un segmento di memoria condivisa, la scrittura e la lettura di un valore intero da questo segmento da parte di processi distinti, e infine la rimozione del segmento di memoria condivisa.

All'inizio, il programma include le necessarie librerie per la gestione dei processi, la manipolazione della memoria condivisa e altre funzioni di sistema. Successivamente, viene generata una chiave unica per il segmento di memoria condivisa utilizzando `ftok`, che trasforma un percorso di file e un byte in una chiave unica.

Con questa chiave, il programma tenta di creare un segmento di memoria condivisa di dimensione sufficiente a contenere un intero (`sizeof(int)`) tramite `shmget`, specificando i permessi di accesso. Se la creazione fallisce, il programma termina stampando un messaggio di errore.

laa riga di codice con `ftok` √® utilizzata per generare una chiave unica (`key_t`) che sar√† poi impiegata per creare o accedere a un segmento di memoria condivisa. 

La funzione `ftok` √® una funzione standard in ambienti Unix e Linux che serve a convertire un percorso di file e un carattere in una chiave unica. Questa chiave √® necessaria per le operazioni successive sulla memoria condivisa, come la creazione o il collegamento a un segmento di memoria condivisa.

Il primo argomento della funzione `ftok`, in questo caso `"."`, indica il percorso del file che verr√† utilizzato come parte della base per la generazione della chiave. Utilizzare `"."` significa che si sta facendo riferimento alla directory corrente. Il secondo argomento, `'A'`, √® un carattere (o pi√π tecnicamente, un intero convertito implicitamente in carattere) che contribuisce a rendere unica la chiave generata. La combinazione di percorso e carattere permette di ridurre la probabilit√† di collisioni con chiavi generate da altri processi che potrebbero utilizzare lo stesso meccanismo.

La chiave generata da `ftok` viene poi assegnata alla variabile `chiave` di tipo `key_t`, che √® un tipo definito specificamente per contenere chiavi IPC (Inter-Process Communication). Questa chiave sar√† utilizzata nelle chiamate successive per identificare univocamente il segmento di memoria condivisa tra diversi processi che intendono comunicare o condividere dati.

Successivamente, il programma si collega al segmento di memoria condivisa ottenendo un puntatore a intero (`int *`) attraverso `shmat`. Se l'operazione fallisce, viene stampato un messaggio di errore.

Il programma poi crea un processo figlio tramite `fork()`. Entrambi i processi, padre e figlio, hanno accesso al puntatore alla memoria condivisa.

Nel processo figlio, viene scritto il valore `123` nel segmento di memoria condivisa e poi il processo si mette in attesa per 3 secondi prima di terminare. Questo ritardo permette al processo padre di leggere il valore dopo che il figlio ha terminato.

Nel processo padre, dopo aver atteso la terminazione del processo figlio con `wait(NULL)`, legge il valore dalla memoria condivisa e lo stampa. Infine, il segmento di memoria condivisa viene rimosso con `shmctl` per evitare perdite di memoria.

Questo esempio illustra un modo efficace per la comunicazione inter-processo (IPC) attraverso la memoria condivisa, permettendo a processi distinti di accedere e modificare dati in un'area di memoria comune.

## Shm Tool.c

[vedi Esempio di programma in C](helloShctool.c) per la manipolazione di segmenti di memoria condivisa in un sistema operativo Unix/Linux. La memoria condivisa √® un metodo efficace per permettere la comunicazione tra processi (IPC, Inter-Process Communication), consentendo loro di accedere a una stessa area di memoria.

All'inizio del codice, vengono incluse diverse librerie standard necessarie per il funzionamento del programma. Queste includono funzioni per l'input/output (`stdio.h`), la manipolazione delle stringhe (`string.h`), definizioni di tipi di dati per i system call (`sys/types.h`), e funzioni specifiche per la gestione della memoria condivisa (`sys/ipc.h` e `sys/shm.h`). Inoltre, viene inclusa la libreria `ctype.h` per la manipolazione dei caratteri.

Il programma definisce una dimensione fissa (`SEGSIZE`) per il segmento di memoria condivisa che intende utilizzare. Successivamente, dichiara una serie di funzioni per le diverse operazioni che possono essere eseguite sul segmento di memoria condivisa: scrittura (`writeshm`), lettura (`readshm`), rimozione (`removeshm`), e cambio dei permessi (`changemode`). Viene anche definita una funzione `usage` per mostrare come utilizzare il programma.

Nella funzione `main`, il programma inizia controllando gli argomenti passati dall'utente. Se non vengono forniti argomenti, viene chiamata la funzione `usage` per mostrare le istruzioni. Il programma procede poi generando una chiave unica per il segmento di memoria condivisa e tenta di creare o accedere a un segmento di memoria condivisa esistente. In caso di successo, il programma si collega al segmento di memoria condivisa e, in base all'argomento fornito dall'utente (`w`, `r`, `d`, `m`), esegue l'operazione richiesta (scrittura, lettura, rimozione, cambio dei permessi).

Le funzioni `writeshm`, `readshm`, `removeshm`, e `changemode` implementano le operazioni specifiche sul segmento di memoria condivisa. `writeshm` copia un testo nel segmento, `readshm` stampa il contenuto del segmento, `removeshm` marca il segmento per la rimozione, e `changemode` cambia i permessi di accesso al segmento.

Questo programma dimostra un utilizzo pratico della memoria condivisa per la comunicazione tra processi, mostrando come creare, accedere, modificare e rimuovere segmenti di memoria condivisa, oltre a gestirne i permessi di accesso.


## MEMORIA Randomizzazione dello spazio degli indirizzi ?

La randomizzazione dello spazio degli indirizzi (Address Space Layout Randomization, ASLR) √® una tecnica di sicurezza utilizzata nei sistemi operativi per prevenire attacchi che sfruttano la corruzione della memoria, come i buffer overflow. ASLR rende difficile per un attaccante predire dove si trovano specifiche parti del codice o dei dati in memoria, randomizzando le posizioni di heap, stack, e librerie a ogni esecuzione del programma.

Nel contesto del codice [`helloMem.c`](helloMem.c"), l'effetto dell'ASLR pu√≤ essere osservato attraverso l'indirizzo di memoria allocato dinamicamente per `p` che viene stampato ogni volta che il programma viene eseguito. 

Se l'ASLR del S.O. √® attivo, l'indirizzo di memoria stampato cambier√† ad ogni esecuzione del programma, riflettendo la randomizzazione dello spazio degli indirizzi. 
Questo comportamento contrasta con quello che si avrebbe se l'ASLR fosse disattivato, dove gli indirizzi di memoria tenderebbero a rimanere consistenti tra le esecuzioni.

Per evidenziare il problema con `helloMem.c` relativo all'ASLR, si pu√≤ procedere nel seguente modo:

1. **Esecuzione Ripetuta**: Eseguire `helloMem.c` pi√π volte e osservare l'indirizzo di memoria stampato per `p`. Se l'indirizzo cambia significativamente tra le esecuzioni, ci√≤ indica che l'ASLR √® attivo.

2. **Confronto con ASLR Disattivato**: Per un confronto diretto, si potrebbe temporaneamente disattivare l'ASLR (se il sistema lo permette e si √® consapevoli dei rischi per la sicurezza) e poi eseguire di nuovo il programma pi√π volte. Se gli indirizzi stampati rimangono consistenti tra le esecuzioni con l'ASLR disattivato, ci√≤ conferma l'effetto dell'ASLR sul programma.

Per disattivare temporaneamente l'ASLR su sistemi Linux, si pu√≤ usare il comando (come root o con sudo):
```bash
echo 0 > /proc/sys/kernel/randomize_va_space
```
E per riattivarlo:
```bash
echo 2 > /proc/sys/kernel/randomize_va_space
```
**Nota**: Modificare le impostazioni dell'ASLR pu√≤ avere implicazioni sulla sicurezza del sistema. √à consigliabile farlo solo in un ambiente controllato e per scopi di test.